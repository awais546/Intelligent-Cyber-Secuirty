{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adversarial Attack Fast Gradient Sign Method.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awais546/Intelligent-Cyber-Secuirty/blob/master/Adversarial_Attack_Fast_Gradient_Sign_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsx_EqJj9R5e",
        "colab_type": "text"
      },
      "source": [
        "**Adversarial Attack on Deep Neural Network Using Fast Gradien Sign Attack (FGSM)**\n",
        "==============================\n",
        "\n",
        "Software security is an important concept and which is considered as one of the most important and essential part of software development. The main goal of a software development is not only to solve real life and customer problems but also make that software secure enough to defend itself from malicious attacks and hacker risks to increase its usability and integrity. \n",
        "\n",
        "**Introduction to Deep Learning**\n",
        "==============================\n",
        "\n",
        "In simple words deep learning can be defined as subfield  of **Machine  Learning**  that can automatically detect  the features/patterns  of data without  **user intervention**. \n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/44099157/70871735-caa31780-1f67-11ea-9ef0-9efca777ce08.JPG)\n",
        "\n",
        "In this current age of technology Deep neural netowrk has rooted itself in almost every major aspect of our lives. The areas of medical diagnostics where deep neural networks are able to diagnose medical conditions with more accuracy than a trained expert. The area of self driving cars,fraud detection, Image processing and many more. \n",
        "\n",
        "Nowadays every individual with a slighly technical background can learn to develope a deep neural network model which can easily predicit the desirable results. Developing these deep neural networks we are forgetting one major and one of the most important concerns. **Is our Deep neural network model safe?** \n",
        "\n",
        "It is without a doubt a fact that the field of deep learning is making our life easier and majority of the fields are becoming dependent on its result, but what if these deep neural networks stop performing as we are expecting it to perform. What if someone is diagnosed with a problem which he does not have or the other way around. These questions raise the concern of security of deep neural networks. \n",
        "\n",
        "As a software can be a victim of malicious attacks and hacker risks so can a deep neural network model. The security of a deep neural network is as much important as any software as the applications of deep learning is effecting our lifes directly.\n",
        "\n",
        "The purpose of this blog is to answer the question that whether a deep nueral network model is safe or not. To answer this question we are going to apply a technique of attack on our model which will mainly focuss on decreasing the efficiency of our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw5nYEHdTh8M",
        "colab_type": "text"
      },
      "source": [
        "**Adversarial Attack**\n",
        "==============================\n",
        " \n",
        " Adversarial attack is a technique used to attack a deep learning model which leads to misclassification of an input (majorly images). \n",
        "\n",
        " The further elaborate this attack we can say that any technique which can decrease the efficiency of a model or deviates the model from its original purpose is an adversarial attack. Simple adversarial attacks include addition of noise in an input image, manipulating the model if you the person has the access to model.\n",
        "\n",
        " The example below shows how a minimal amount of noise can effect the classification of a person.\n",
        "\n",
        " ![alt text](https://user-images.githubusercontent.com/44099157/70873744-a00b8b80-1f74-11ea-9c20-6545eb4f6c18.JPG)\n",
        "\n",
        " In this blog I am going to use a type of adversarial attack called **FGSM** and try to attack a state of the art deep neural network model to misclassify the input images.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEjjMy9octqV",
        "colab_type": "text"
      },
      "source": [
        "**Fast Gradient Sign Method**\n",
        "===================\n",
        "\n",
        "In FGSM the model is attacked using and adversarial image which is slightly modified in order to fool the classifier (misclassification). As the name indicates this adversarial attack focusses on Gradient (slope of loss function). A deep learning works by finding the global minima of an image in order to get the minimum loss. FGSM works on the same principle but instead of getting the global minima we deviate it from the global minima by the addition of Gradient in each pixel of the image. \n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/44099157/70931441-1c49b180-1ffd-11ea-8794-6697d56fdb46.JPG)\n",
        "\n",
        "In a nut shell this technique leverage the way of learning gradient and adjust the weight (backpropagation) to maximize the loss instead of minimizing it. \n",
        "To generate a new image (noisy image) by the addition of gradient can be done using the following formula.\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/44099157/71060627-9e37f880-212b-11ea-99ec-d4a2d54c349a.JPG)\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/44099157/71060771-fff86280-212b-11ea-9ece-058789edb275.JPG)\n",
        "\n",
        "Before we move ahead to code its implementation lets view the famous Panda example.\n",
        "\n",
        "![alt text](https://pytorch.org/tutorials/_images/fgsm_panda_image.png).\n",
        "\n",
        "Figrue shows that x correctly classified input of panda, y is the ground truth or real lable of image x, θ  represents the model parameters, and  J(θ,x,y)  is the loss that is used to train the network.∇xJ(θ,x,y) is calculated when FGSM attack backpropagates the gradient back to input. ( ϵ  or  0.007  in the picture) in the direction (i.e.  sign(∇xJ(θ,x,y)) ) that will maximize the loss. The resulting output image x′ is the wrongly classfied label as \"Gibbon\" when it is still clearly a \"Panda\"\n",
        "\n",
        "In order to implement this technique we are going to use **MNIST dataset**. `"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRC25QNwsSM7",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "==========\n",
        "\n",
        "For the implementation of this attack we are going to use Pytorch. In order to check the effect of epsilon we are taking a list of epsilon in order to analyze the behavior of epsilon on the image. For this example we are using a pretrained **Lenet Model** that has an accuracy of **98%** on MNIST dataset. In order to download this pretrained model use the following link.\n",
        "\n",
        "[Lenet Pretrained Model](https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmwJRTOEtyhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "pretrained_model = \"/content/drive/My Drive/Colab Notebooks/lenet_mnist_model.pth\"\n",
        "use_cuda=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGVseh7kuqCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5 )\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iKY5HrZuuKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST Test dataset and dataloader declaration\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "              download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "              ])), batch_size=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochWeStCu80C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the network\n",
        "model = Net()\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_state_dict(torch.load(pretrained_model))\n",
        "\n",
        "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-PZSQA9vQG0",
        "colab_type": "text"
      },
      "source": [
        "The FGSM equation can be implemented in the following way.\n",
        "\n",
        "\\begin{align}perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\end{align}\n",
        "\n",
        "In order to maintain the original range of the data, the perturbed image is clipped to range  [0,1] ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxGyMgkNvoLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    \n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW8yFmcywILX",
        "colab_type": "text"
      },
      "source": [
        "**Test Funtion**\n",
        "=========\n",
        "\n",
        "The test function below takes the value of epsilon and produce the accuracy by running a full test step on MNIST datasest. For each sample in the dataset the function calculates the gradient of the loss w.r.t input data **(data_grad)** and creates a nosiy image by calling a fgsm_attack and checks if the image is adversarial or not (produced a wrong label or not). In order to see which images turned out to be adversarial I am also saving some examples of successful adversarial images to show as a result of attack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLubSt0Tx6NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader, epsilon):\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "        if init_pred.item() != target.item():\n",
        "            continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = model(perturbed_data)\n",
        "\n",
        "        # Check for success\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
        "                adv_ex = perturbed_data.squeeze().detach().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "        else:\n",
        "            # Save some adv examples for visualization later\n",
        "            if len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, adv_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4rc5Q8ayBxl",
        "colab_type": "text"
      },
      "source": [
        "**Attack**\n",
        "========"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44kR6xJayFro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "examples = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons:\n",
        "    acc, ex = test(model, test_loader, eps)\n",
        "    accuracies.append(acc)\n",
        "    examples.append(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuQygzm3yLhZ",
        "colab_type": "text"
      },
      "source": [
        "**Results**\n",
        "========\n",
        "\n",
        "From the graph given below we can clearly see that increasing the epsilon is decreasing the efficiency of the model drastically because larger epsilons mean we take a larger step in the direction that will maximize the loss. **Notice the trend in the curve is not linear even though the epsilon values are linearly spaced.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeaxDsHzyr3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(epsilons, accuracies, \"*-\")\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.xticks(np.arange(0, .35, step=0.05))\n",
        "plt.title(\"Accuracy vs Epsilon\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KvsYmazJYJ",
        "colab_type": "text"
      },
      "source": [
        "**Sample Test Results**\n",
        "========\n",
        "\n",
        "Below we show some examples of successful adversarial examples at each epsilon value. Each row of the plot shows a different epsilon value. The first row is the  ϵ=0  examples which represent the original “clean” images with no perturbation. The title of each image shows the “original classification -> adversarial classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JcyM72tzTvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot several examples of adversarial samples at each epsilon\n",
        "cnt = 0\n",
        "plt.figure(figsize=(8,10))\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(examples[i])):\n",
        "        cnt += 1\n",
        "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        if j == 0:\n",
        "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
        "        orig,adv,ex = examples[i][j]\n",
        "        plt.title(\"{} -> {}\".format(orig, adv))\n",
        "        plt.imshow(ex, cmap=\"gray\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ1jCytzXXf",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion**\n",
        "=======\n",
        "\n",
        "From the above implementation we can clearly see that deep neural network are prone to attack. In the above example we have attacked a state of the art model whose accuracy was **98%** but a little addition of noise has drastically decreased this percentage. Similarly we can attack other models too which are being used in our practical lives. This raised our concerns that wheter a deep neural network is safe or not. If it is not safe then how can be improve its security as these models have an immense effect on us."
      ]
    }
  ]
}